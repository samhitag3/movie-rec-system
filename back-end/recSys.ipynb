{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip3 install ipykernel torch torchvision torchaudio pandas scikit-learn flask cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, metrics, preprocessing\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie dataframe dimensions: (9742, 3)\n",
      "Ratings dataframe dimensions: (100836, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964981247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964983815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "      <td>964982931</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating  timestamp\n",
       "0       1        1     4.0  964982703\n",
       "1       1        3     4.0  964981247\n",
       "2       1        6     4.0  964982224\n",
       "3       1       47     5.0  964983815\n",
       "4       1       50     5.0  964982931"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in data from csv files\n",
    "movies_df = pd.read_csv(\"./Data/ml-latest-small/movies.csv\")\n",
    "ratings_df = pd.read_csv(\"./Data/ml-latest-small/ratings.csv\")\n",
    "\n",
    "print(f\"Movie dataframe dimensions: {movies_df.shape}\")\n",
    "print(f\"Ratings dataframe dimensions: {ratings_df.shape}\")\n",
    "\n",
    "# get number of unique users and movies\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())\n",
    "\n",
    "ratings_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Classes Needed for PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user and item embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.movie_factors = torch.nn.Embedding(n_items, n_factors)\n",
    "        # fills weights with values from a uniform distribution [0, 0.5]\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.movie_factors.weight.data.uniform_(0, 0.05)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # matrix multiplication between user and item factors, and then concatenates them to one column\n",
    "        return (self.user_factors(data[:,0])*self.movie_factors(data[:,1])).sum(1)\n",
    "    \n",
    "    def predict(self, user, item):\n",
    "        return (self.user_factors(user)*self.movie_factors(item)).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings = ratings\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(self.ratings.movieId.values)\n",
    "        self.lookup = dict(zip(le.transform(self.ratings.movieId.values), self.ratings.movieId.values))\n",
    "\n",
    "        self.ratings.userId = preprocessing.LabelEncoder().fit_transform(self.ratings.userId.values)\n",
    "        self.ratings.movieId = preprocessing.LabelEncoder().fit_transform(self.ratings.movieId.values)\n",
    "\n",
    "        self.x = torch.tensor(self.ratings.drop(['rating', 'timestamp'], axis=1).values)\n",
    "        self.y = torch.tensor(self.ratings['rating'].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (self.x[item], self.y[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: False\n",
      "user_factors.weight tensor([[0.0414, 0.0116, 0.0377,  ..., 0.0225, 0.0053, 0.0193],\n",
      "        [0.0230, 0.0121, 0.0220,  ..., 0.0478, 0.0443, 0.0261],\n",
      "        [0.0487, 0.0319, 0.0065,  ..., 0.0160, 0.0081, 0.0450],\n",
      "        ...,\n",
      "        [0.0448, 0.0048, 0.0022,  ..., 0.0335, 0.0283, 0.0262],\n",
      "        [0.0305, 0.0256, 0.0112,  ..., 0.0350, 0.0448, 0.0079],\n",
      "        [0.0027, 0.0127, 0.0356,  ..., 0.0005, 0.0011, 0.0232]])\n",
      "movie_factors.weight tensor([[0.0440, 0.0252, 0.0293,  ..., 0.0262, 0.0207, 0.0034],\n",
      "        [0.0412, 0.0053, 0.0209,  ..., 0.0428, 0.0488, 0.0186],\n",
      "        [0.0180, 0.0401, 0.0086,  ..., 0.0015, 0.0429, 0.0120],\n",
      "        ...,\n",
      "        [0.0090, 0.0492, 0.0409,  ..., 0.0138, 0.0240, 0.0168],\n",
      "        [0.0492, 0.0129, 0.0296,  ..., 0.0206, 0.0291, 0.0290],\n",
      "        [0.0319, 0.0390, 0.0051,  ..., 0.0097, 0.0243, 0.0490]])\n",
      "iter #0 Loss: 11.062116707642065\n",
      "iter #1 Loss: 4.739447764938858\n",
      "iter #2 Loss: 2.473496480036508\n",
      "iter #3 Loss: 1.7203255179267245\n",
      "iter #4 Loss: 1.345202693252394\n",
      "iter #5 Loss: 1.1284910246049087\n",
      "iter #6 Loss: 0.9912855743151631\n",
      "iter #7 Loss: 0.9001586078689788\n",
      "iter #8 Loss: 0.8370559593747715\n",
      "iter #9 Loss: 0.7920801935310896\n",
      "iter #10 Loss: 0.7592315034666642\n",
      "iter #11 Loss: 0.734889231054916\n",
      "iter #12 Loss: 0.7158252089307998\n",
      "iter #13 Loss: 0.7016760347609593\n",
      "iter #14 Loss: 0.6904399603000148\n",
      "iter #15 Loss: 0.6816466264615809\n",
      "iter #16 Loss: 0.674788431694665\n",
      "iter #17 Loss: 0.6695029616431536\n",
      "iter #18 Loss: 0.6659159803677936\n",
      "iter #19 Loss: 0.6628610803769325\n",
      "iter #20 Loss: 0.6607149414680331\n",
      "iter #21 Loss: 0.6589294934862762\n",
      "iter #22 Loss: 0.657909057315836\n",
      "iter #23 Loss: 0.6569124790390736\n",
      "iter #24 Loss: 0.6558870558962604\n",
      "iter #25 Loss: 0.655271877340859\n",
      "iter #26 Loss: 0.6547338612339824\n",
      "iter #27 Loss: 0.6538181602198461\n",
      "iter #28 Loss: 0.6531800482857045\n",
      "iter #29 Loss: 0.6519594393708379\n",
      "iter #30 Loss: 0.6508535589209667\n",
      "iter #31 Loss: 0.6494448457650727\n",
      "iter #32 Loss: 0.6470976290742153\n",
      "iter #33 Loss: 0.6452693484988309\n",
      "iter #34 Loss: 0.642402324050211\n",
      "iter #35 Loss: 0.6387318436642588\n",
      "iter #36 Loss: 0.6350425562353303\n",
      "iter #37 Loss: 0.6302168737283818\n",
      "iter #38 Loss: 0.6251932322071289\n",
      "iter #39 Loss: 0.6182650190016945\n",
      "iter #40 Loss: 0.6111582699917295\n",
      "iter #41 Loss: 0.6031075199394662\n",
      "iter #42 Loss: 0.594893714800704\n",
      "iter #43 Loss: 0.5855967285593754\n",
      "iter #44 Loss: 0.5758059794451985\n",
      "iter #45 Loss: 0.56588832750387\n",
      "iter #46 Loss: 0.5562470225135082\n",
      "iter #47 Loss: 0.545975546016911\n",
      "iter #48 Loss: 0.5358660838325616\n",
      "iter #49 Loss: 0.5261752570265441\n",
      "iter #50 Loss: 0.5161627797305887\n",
      "iter #51 Loss: 0.5068110748369077\n",
      "iter #52 Loss: 0.49757088917463566\n",
      "iter #53 Loss: 0.4888029822767688\n",
      "iter #54 Loss: 0.4807713764724393\n",
      "iter #55 Loss: 0.4730230940855699\n",
      "iter #56 Loss: 0.4656359340409337\n",
      "iter #57 Loss: 0.45898745205196634\n",
      "iter #58 Loss: 0.4525995783999487\n",
      "iter #59 Loss: 0.44638059053778045\n",
      "iter #60 Loss: 0.4410691970609469\n",
      "iter #61 Loss: 0.43547800510440987\n",
      "iter #62 Loss: 0.43046060382260887\n",
      "iter #63 Loss: 0.42567300282153986\n",
      "iter #64 Loss: 0.42154369565662997\n",
      "iter #65 Loss: 0.41727594106557403\n",
      "iter #66 Loss: 0.4131737453153896\n",
      "iter #67 Loss: 0.40932177680398\n",
      "iter #68 Loss: 0.4058228892269473\n",
      "iter #69 Loss: 0.402630620555678\n",
      "iter #70 Loss: 0.3991818003129535\n",
      "iter #71 Loss: 0.39622516568920335\n",
      "iter #72 Loss: 0.39324568457863657\n",
      "iter #73 Loss: 0.39049689925564124\n",
      "iter #74 Loss: 0.3879604133668587\n",
      "iter #75 Loss: 0.38521673793190625\n",
      "iter #76 Loss: 0.3828976038030259\n",
      "iter #77 Loss: 0.38047039535293725\n",
      "iter #78 Loss: 0.3780368190958415\n",
      "iter #79 Loss: 0.3760651696076248\n",
      "iter #80 Loss: 0.37382227290175896\n",
      "iter #81 Loss: 0.37213840435831075\n",
      "iter #82 Loss: 0.37021298969381955\n",
      "iter #83 Loss: 0.3682027990140286\n",
      "iter #84 Loss: 0.36639390445224523\n",
      "iter #85 Loss: 0.36483441680896705\n",
      "iter #86 Loss: 0.3629211620971334\n",
      "iter #87 Loss: 0.3614003147910997\n",
      "iter #88 Loss: 0.3599951792943296\n",
      "iter #89 Loss: 0.35855548390125863\n",
      "iter #90 Loss: 0.3572091682421677\n",
      "iter #91 Loss: 0.35550481421390767\n",
      "iter #92 Loss: 0.3543152575414193\n",
      "iter #93 Loss: 0.35303709083024015\n",
      "iter #94 Loss: 0.3518823711627026\n",
      "iter #95 Loss: 0.35068281652963706\n",
      "iter #96 Loss: 0.34949015173588305\n",
      "iter #97 Loss: 0.3485901211459322\n",
      "iter #98 Loss: 0.34737094499284243\n",
      "iter #99 Loss: 0.34654288534736877\n",
      "iter #100 Loss: 0.34531308950733414\n",
      "iter #101 Loss: 0.34437788767636124\n",
      "iter #102 Loss: 0.3434202092969176\n",
      "iter #103 Loss: 0.3425927348056723\n",
      "iter #104 Loss: 0.34176795203522375\n",
      "iter #105 Loss: 0.3406344448324993\n",
      "iter #106 Loss: 0.3399811982352116\n",
      "iter #107 Loss: 0.3389572494374919\n",
      "iter #108 Loss: 0.3385113968802285\n",
      "iter #109 Loss: 0.33762828646426274\n",
      "iter #110 Loss: 0.336834683220096\n",
      "iter #111 Loss: 0.3360078424442238\n",
      "iter #112 Loss: 0.33544178233307026\n",
      "iter #113 Loss: 0.33474351894961396\n",
      "iter #114 Loss: 0.33410671324974994\n",
      "iter #115 Loss: 0.33351019629972234\n",
      "iter #116 Loss: 0.3327798027223742\n",
      "iter #117 Loss: 0.33235826642077587\n",
      "iter #118 Loss: 0.33165062201885404\n",
      "iter #119 Loss: 0.3308936583095698\n",
      "iter #120 Loss: 0.3306149731054524\n",
      "iter #121 Loss: 0.3299709095945818\n",
      "iter #122 Loss: 0.32929334986179615\n",
      "iter #123 Loss: 0.3287956764363698\n",
      "iter #124 Loss: 0.328451742810495\n",
      "iter #125 Loss: 0.3278527861167937\n",
      "iter #126 Loss: 0.3272447876518753\n",
      "iter #127 Loss: 0.3268267755127195\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = Model(n_users, n_items, n_factors=8)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # prints the parameters who's changes will be recorded\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "# enable GPU if you have a GPU\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# ADAM optimizier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train data\n",
    "train_set = MovieDataset(ratings_df)\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)\n",
    "\n",
    "for it in range(num_epochs):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[ 1.5459,  1.5626,  0.3918,  ...,  1.3266,  1.2053,  1.6495],\n",
      "        [ 1.4505,  0.5052,  1.0755,  ...,  0.2588,  0.9375,  1.3744],\n",
      "        [ 1.4311, -0.3158, -0.4011,  ..., -0.8710, -2.2422,  2.6228],\n",
      "        ...,\n",
      "        [ 1.5472,  2.0237,  0.2342,  ...,  1.4624,  0.1823,  0.4366],\n",
      "        [ 0.7654,  1.7095,  0.6038,  ...,  0.7168,  0.8237,  0.9143],\n",
      "        [ 1.1587,  0.0038,  1.3299,  ...,  1.0626,  1.2879,  1.7076]])\n",
      "movie_factors.weight tensor([[0.4595, 0.2566, 0.7599,  ..., 0.4822, 0.8101, 0.1779],\n",
      "        [0.5607, 0.1511, 0.6720,  ..., 0.0382, 0.3733, 0.4972],\n",
      "        [0.2896, 0.7683, 0.5279,  ..., 0.2558, 0.1718, 0.4468],\n",
      "        ...,\n",
      "        [0.4181, 0.4432, 0.4126,  ..., 0.4293, 0.4070, 0.4155],\n",
      "        [0.4134, 0.4182, 0.4095,  ..., 0.4095, 0.4085, 0.3807],\n",
      "        [0.4662, 0.5197, 0.5108,  ..., 0.5193, 0.5188, 0.4895]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('recSys.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for getting top n recommendations for a certain user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Tango in Paris (Ultimo tango a Parigi) (1972)\n",
      "Sausage Party (2016)\n",
      "Galaxy of Terror (Quest) (1981)\n",
      "Repulsion (1965)\n",
      "Half Nelson (2006)\n",
      "Kung Pow: Enter the Fist (2002)\n",
      "Looker (1981)\n",
      "Alien Contamination (1980)\n",
      "Master of the Flying Guillotine (Du bi quan wang da po xue di zi) (1975)\n",
      "Shall We Dance (1937)\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_recommendations(i, n):\n",
    "    # Load in data from csv files\n",
    "    actual_ratings = pd.read_csv(\"./Data/ml-latest-small/ratings.csv\")\n",
    "    actual_ratings = actual_ratings.pivot(index='userId', columns='movieId', values='rating').fillna(-1)\n",
    "    predicted_ratings = torch.matmul(model.user_factors.weight.data, model.movie_factors.weight.data.T)\n",
    "\n",
    "    # i < actual_ratings.shape[0]\n",
    "\n",
    "    recommendations = []\n",
    "    rated = []\n",
    "    for j in range(actual_ratings.shape[1]):\n",
    "        if actual_ratings.iloc[i][actual_ratings.columns[j]] > 0:\n",
    "            rated.append((actual_ratings.columns[j], actual_ratings.iloc[i][actual_ratings.columns[j]], float(predicted_ratings[i][j])))\n",
    "        else:\n",
    "            heapq.heappush(recommendations, (-float(predicted_ratings[i][j]), actual_ratings.columns[j]))\n",
    "    # difference between predicted and rated: sum(list(map(lambda x: (x[1] - x[2])**2, rated)))\n",
    "    movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "    for x in range(n):\n",
    "        print(movie_names[recommendations[x][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Running a clustering algorithm on our result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Swimming with Sharks (1995)\n",
      "\t My Fair Lady (1964)\n",
      "\t Philadelphia Story, The (1940)\n",
      "\t High Noon (1952)\n",
      "\t Maltese Falcon, The (1941)\n",
      "\t Snow White and the Seven Dwarfs (1937)\n",
      "\t Real Genius (1985)\n",
      "\t Safe (1995)\n",
      "\t Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)\n",
      "\t Speechless (1994)\n",
      "Cluster #1\n",
      "\t Restoration (1995)\n",
      "\t What's Eating Gilbert Grape (1993)\n",
      "\t Dangerous Minds (1995)\n",
      "\t Puppet Masters, The (1994)\n",
      "\t Some Kind of Wonderful (1987)\n",
      "\t Sudden Death (1995)\n",
      "\t New Guy, The (2002)\n",
      "\t Gaslight (1944)\n",
      "\t Jennifer 8 (1992)\n",
      "\t Specialist, The (1994)\n",
      "Cluster #2\n",
      "\t Hate (Haine, La) (1995)\n",
      "\t Chungking Express (Chung Hing sam lam) (1994)\n",
      "\t Singin' in the Rain (1952)\n",
      "\t Orlando (1992)\n",
      "\t Afterglow (1997)\n",
      "\t Unforgiven (1992)\n",
      "\t Candleshoe (1977)\n",
      "\t Empire (2002)\n",
      "\t Penny Serenade (1941)\n",
      "\t North by Northwest (1959)\n",
      "Cluster #3\n",
      "\t Miracle on 34th Street (1994)\n",
      "\t Just Cause (1995)\n",
      "\t Go Fish (1994)\n",
      "\t How to Make an American Quilt (1995)\n",
      "\t Piano, The (1993)\n",
      "\t Mixed Nuts (1994)\n",
      "\t NeverEnding Story III, The (1994)\n",
      "\t Out Cold (2001)\n",
      "\t I'll Do Anything (1994)\n",
      "\t Insider, The (1999)\n",
      "Cluster #4\n",
      "\t On Her Majesty's Secret Service (1969)\n",
      "\t Run Silent Run Deep (1958)\n",
      "\t My Dog Skip (1999)\n",
      "\t Color of Night (1994)\n",
      "\t American Werewolf in London, An (1981)\n",
      "\t Cement Garden, The (1993)\n",
      "\t To Gillian on Her 37th Birthday (1996)\n",
      "\t For Richer or Poorer (1997)\n",
      "\t Simple Plan, A (1998)\n",
      "\t Casino (1995)\n",
      "Cluster #5\n",
      "\t Poetic Justice (1993)\n",
      "\t Lost Weekend, The (1945)\n",
      "\t Being Human (1993)\n",
      "\t For Whom the Bell Tolls (1943)\n",
      "\t Sixteen Candles (1984)\n",
      "\t Bread and Chocolate (Pane e cioccolata) (1973)\n",
      "\t Kissed (1996)\n",
      "\t Journey of Natty Gann, The (1985)\n",
      "\t Philadelphia (1993)\n",
      "\t Queen Margot (Reine Margot, La) (1994)\n",
      "Cluster #6\n",
      "\t Secret of Roan Inish, The (1994)\n",
      "\t Some Like It Hot (1959)\n",
      "\t Vanya on 42nd Street (1994)\n",
      "\t Night of the Living Dead (1968)\n",
      "\t Gigi (1958)\n",
      "\t My Favorite Year (1982)\n",
      "\t Naked Gun: From the Files of Police Squad!, The (1988)\n",
      "\t Living in Oblivion (1995)\n",
      "\t Three Colors: Blue (Trois couleurs: Bleu) (1993)\n",
      "\t Bubba Ho-tep (2002)\n",
      "Cluster #7\n",
      "\t Unlawful Entry (1992)\n",
      "\t Friday the 13th Part V: A New Beginning (1985)\n",
      "\t Crow: Salvation, The (2000)\n",
      "\t Everybody's Famous! (Iedereen beroemd!) (2000)\n",
      "\t Godzilla (1998)\n",
      "\t Undercover Blues (1993)\n",
      "\t Psycho III (1986)\n",
      "\t Extreme Ops (2002)\n",
      "\t Jane Austen's Mafia! (1998)\n",
      "\t Masquerade (1988)\n",
      "Cluster #8\n",
      "\t Don Juan DeMarco (1995)\n",
      "\t Making Mr. Right (1987)\n",
      "\t Robin Hood: Men in Tights (1993)\n",
      "\t Super Mario Bros. (1993)\n",
      "\t National Lampoon's Senior Trip (1995)\n",
      "\t Ref, The (1994)\n",
      "\t Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
      "\t Funny Face (1957)\n",
      "\t Love and Death (1975)\n",
      "\t Interview with the Vampire: The Vampire Chronicles (1994)\n",
      "Cluster #9\n",
      "\t Perfect World, A (1993)\n",
      "\t Spy Who Loved Me, The (1977)\n",
      "\t North (1994)\n",
      "\t Down to Earth (2001)\n",
      "\t Babysitter, The (1995)\n",
      "\t Father of the Bride Part II (1995)\n",
      "\t Nightmare on Elm Street 4: The Dream Master, A (1988)\n",
      "\t Cutthroat Island (1995)\n",
      "\t Milk Money (1994)\n",
      "\t Bulletproof Monk (2003)\n"
     ]
    }
   ],
   "source": [
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "trained_movie_embeddings = model.movie_factors.weight.data.cpu().numpy()\n",
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)\n",
    "for cluster in range(10):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  movs = []\n",
    "  for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    movid = train_set.lookup[movidx]\n",
    "    # print(ratings_df.loc[ratings_df['movieId']==movid].count())\n",
    "    rat_count = ratings_df.loc[ratings_df['movieId']==movid].count()[\"userId\"]\n",
    "    movs.append((movie_names[movid], rat_count))\n",
    "  for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "    print(\"\\t\", mov[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
