{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection, metrics, preprocessing\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie dataframe dimensions: (9742, 3)\n",
      "Ratings dataframe dimensions: (100836, 4)\n"
     ]
    }
   ],
   "source": [
    "# Load in data from csv files\n",
    "movies_df = pd.read_csv(\"./Data/ml-latest-small/movies.csv\")\n",
    "ratings_df = pd.read_csv(\"./Data/ml-latest-small/ratings.csv\")\n",
    "\n",
    "print(f\"Movie dataframe dimensions: {movies_df.shape}\")\n",
    "print(f\"Ratings dataframe dimensions: {ratings_df.shape}\")\n",
    "\n",
    "# get number of unique users and movies\n",
    "n_users = len(ratings_df.userId.unique())\n",
    "n_items = len(ratings_df.movieId.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_factors=20):\n",
    "        super().__init__()\n",
    "        # create user and item embeddings\n",
    "        self.user_factors = torch.nn.Embedding(n_users, n_factors)\n",
    "        self.movie_factors = torch.nn.Embedding(n_items, n_factors)\n",
    "        # fills weights with values from a uniform distribution [0, 0.5]\n",
    "        self.user_factors.weight.data.uniform_(0, 0.05)\n",
    "        self.movie_factors.weight.data.uniform_(0, 0.05)\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # matrix multiplication between user and item factors, and then concatenates them to one column\n",
    "        return (self.user_factors(data[:,0])*self.movie_factors(data[:,1])).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, ratings):\n",
    "        self.ratings = ratings\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        le.fit(self.ratings.movieId.values)\n",
    "        self.lookup = dict(zip(le.transform(self.ratings.movieId.values), self.ratings.movieId.values))\n",
    "\n",
    "        self.ratings.userId = preprocessing.LabelEncoder().fit_transform(self.ratings.userId.values)\n",
    "        self.ratings.movieId = preprocessing.LabelEncoder().fit_transform(self.ratings.movieId.values)\n",
    "\n",
    "        self.x = torch.tensor(self.ratings.drop(['rating', 'timestamp'], axis=1).values)\n",
    "        self.y = torch.tensor(self.ratings['rating'].values)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        return (self.x[item], self.y[item])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is running on GPU: False\n",
      "user_factors.weight tensor([[0.0015, 0.0462, 0.0324,  ..., 0.0005, 0.0394, 0.0298],\n",
      "        [0.0207, 0.0359, 0.0452,  ..., 0.0371, 0.0230, 0.0445],\n",
      "        [0.0248, 0.0325, 0.0300,  ..., 0.0028, 0.0499, 0.0222],\n",
      "        ...,\n",
      "        [0.0220, 0.0003, 0.0228,  ..., 0.0166, 0.0177, 0.0013],\n",
      "        [0.0425, 0.0317, 0.0094,  ..., 0.0226, 0.0093, 0.0479],\n",
      "        [0.0383, 0.0435, 0.0182,  ..., 0.0381, 0.0365, 0.0190]])\n",
      "movie_factors.weight tensor([[0.0389, 0.0198, 0.0426,  ..., 0.0156, 0.0431, 0.0231],\n",
      "        [0.0328, 0.0334, 0.0264,  ..., 0.0316, 0.0387, 0.0369],\n",
      "        [0.0035, 0.0332, 0.0034,  ..., 0.0139, 0.0464, 0.0207],\n",
      "        ...,\n",
      "        [0.0360, 0.0096, 0.0336,  ..., 0.0431, 0.0266, 0.0499],\n",
      "        [0.0012, 0.0092, 0.0028,  ..., 0.0315, 0.0219, 0.0266],\n",
      "        [0.0248, 0.0386, 0.0230,  ..., 0.0277, 0.0300, 0.0012]])\n",
      "iter #0 Loss: 11.048663442510033\n",
      "iter #1 Loss: 4.7379556746047164\n",
      "iter #2 Loss: 2.474569682391162\n",
      "iter #3 Loss: 1.7211123744243293\n",
      "iter #4 Loss: 1.3460805582516084\n",
      "iter #5 Loss: 1.1286446513409543\n",
      "iter #6 Loss: 0.9916724605759993\n",
      "iter #7 Loss: 0.9003788410558313\n",
      "iter #8 Loss: 0.8373026673110003\n",
      "iter #9 Loss: 0.792052656487765\n",
      "iter #10 Loss: 0.7595968870493361\n",
      "iter #11 Loss: 0.7348983780230363\n",
      "iter #12 Loss: 0.7161561544927849\n",
      "iter #13 Loss: 0.7015088232716328\n",
      "iter #14 Loss: 0.6904926772045\n",
      "iter #15 Loss: 0.6817094106889013\n",
      "iter #16 Loss: 0.6750461236020635\n",
      "iter #17 Loss: 0.6698753644669722\n",
      "iter #18 Loss: 0.665861305738161\n",
      "iter #19 Loss: 0.6629844695178385\n",
      "iter #20 Loss: 0.6607272375356122\n",
      "iter #21 Loss: 0.6591087996505843\n",
      "iter #22 Loss: 0.657348245760511\n",
      "iter #23 Loss: 0.6565256775711393\n",
      "iter #24 Loss: 0.6560703145746652\n",
      "iter #25 Loss: 0.6551314798103371\n",
      "iter #26 Loss: 0.6541504142475008\n",
      "iter #27 Loss: 0.6536092452591445\n",
      "iter #28 Loss: 0.6527610498712147\n",
      "iter #29 Loss: 0.6513838729428761\n",
      "iter #30 Loss: 0.6503093364048125\n",
      "iter #31 Loss: 0.6487447859974682\n",
      "iter #32 Loss: 0.6468757163676514\n",
      "iter #33 Loss: 0.6442364840791915\n",
      "iter #34 Loss: 0.6415445934319254\n",
      "iter #35 Loss: 0.6380356882656286\n",
      "iter #36 Loss: 0.6341433131377104\n",
      "iter #37 Loss: 0.6293386330988806\n",
      "iter #38 Loss: 0.623598314254417\n",
      "iter #39 Loss: 0.6176252377592972\n",
      "iter #40 Loss: 0.6106320269746224\n",
      "iter #41 Loss: 0.6030554131127251\n",
      "iter #42 Loss: 0.595259225421448\n",
      "iter #43 Loss: 0.5868134060260003\n",
      "iter #44 Loss: 0.5780253929232583\n",
      "iter #45 Loss: 0.568972294630133\n",
      "iter #46 Loss: 0.5601271690072747\n",
      "iter #47 Loss: 0.550669412171175\n",
      "iter #48 Loss: 0.5421725330013915\n",
      "iter #49 Loss: 0.5331944944667937\n",
      "iter #50 Loss: 0.5246206759120607\n",
      "iter #51 Loss: 0.5161508893981803\n",
      "iter #52 Loss: 0.5080425252164076\n",
      "iter #53 Loss: 0.500248362986267\n",
      "iter #54 Loss: 0.4924470798164455\n",
      "iter #55 Loss: 0.4854751807557145\n",
      "iter #56 Loss: 0.47850603039192063\n",
      "iter #57 Loss: 0.4719645116232373\n",
      "iter #58 Loss: 0.4655367310730939\n",
      "iter #59 Loss: 0.45957935582562753\n",
      "iter #60 Loss: 0.45364610943470507\n",
      "iter #61 Loss: 0.4484425086887355\n",
      "iter #62 Loss: 0.4429724537932933\n",
      "iter #63 Loss: 0.4380703796136198\n",
      "iter #64 Loss: 0.4335856292831716\n",
      "iter #65 Loss: 0.42891683291435845\n",
      "iter #66 Loss: 0.42482107202640645\n",
      "iter #67 Loss: 0.42052896250852473\n",
      "iter #68 Loss: 0.41684860847701277\n",
      "iter #69 Loss: 0.41305749693195226\n",
      "iter #70 Loss: 0.40945772803979474\n",
      "iter #71 Loss: 0.40605812920682927\n",
      "iter #72 Loss: 0.4028423601941106\n",
      "iter #73 Loss: 0.399632809220383\n",
      "iter #74 Loss: 0.3967036564980969\n",
      "iter #75 Loss: 0.3939596419860869\n",
      "iter #76 Loss: 0.3912548254324397\n",
      "iter #77 Loss: 0.3884841045601114\n",
      "iter #78 Loss: 0.3859773127333767\n",
      "iter #79 Loss: 0.38363747391377\n",
      "iter #80 Loss: 0.38143466881991644\n",
      "iter #81 Loss: 0.3790365755179812\n",
      "iter #82 Loss: 0.3772119632378447\n",
      "iter #83 Loss: 0.3749547032875761\n",
      "iter #84 Loss: 0.37318532236941576\n",
      "iter #85 Loss: 0.3712612786684847\n",
      "iter #86 Loss: 0.3692809488567604\n",
      "iter #87 Loss: 0.367593601724218\n",
      "iter #88 Loss: 0.36583180128015236\n",
      "iter #89 Loss: 0.36452322973288254\n",
      "iter #90 Loss: 0.3630841493341826\n",
      "iter #91 Loss: 0.3615315194965014\n",
      "iter #92 Loss: 0.35994983914539896\n",
      "iter #93 Loss: 0.358555028640528\n",
      "iter #94 Loss: 0.35718263596825794\n",
      "iter #95 Loss: 0.3560699120466479\n",
      "iter #96 Loss: 0.35468780198299943\n",
      "iter #97 Loss: 0.3536258738506869\n",
      "iter #98 Loss: 0.3524704974163607\n",
      "iter #99 Loss: 0.3513361609488877\n",
      "iter #100 Loss: 0.3502808650057328\n",
      "iter #101 Loss: 0.3493804731004432\n",
      "iter #102 Loss: 0.34812379253999837\n",
      "iter #103 Loss: 0.3475087523309107\n",
      "iter #104 Loss: 0.346326648394771\n",
      "iter #105 Loss: 0.3454870272824909\n",
      "iter #106 Loss: 0.3445941043384184\n",
      "iter #107 Loss: 0.3437612948910839\n",
      "iter #108 Loss: 0.34284742492331466\n",
      "iter #109 Loss: 0.34205450369092416\n",
      "iter #110 Loss: 0.34134975155144176\n",
      "iter #111 Loss: 0.34031760821185136\n",
      "iter #112 Loss: 0.33986938122641014\n",
      "iter #113 Loss: 0.33915926865892965\n",
      "iter #114 Loss: 0.3383543091414846\n",
      "iter #115 Loss: 0.3376669863872419\n",
      "iter #116 Loss: 0.33697151672567816\n",
      "iter #117 Loss: 0.3363129386706703\n",
      "iter #118 Loss: 0.3357169308940771\n",
      "iter #119 Loss: 0.33501488414739594\n",
      "iter #120 Loss: 0.334581338909223\n",
      "iter #121 Loss: 0.3339737915259025\n",
      "iter #122 Loss: 0.33352791370474144\n",
      "iter #123 Loss: 0.33276216216801385\n",
      "iter #124 Loss: 0.33234736243934193\n",
      "iter #125 Loss: 0.33185673559907125\n",
      "iter #126 Loss: 0.33109846280008404\n",
      "iter #127 Loss: 0.3306373240530188\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 128\n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "print(\"Is running on GPU:\", cuda)\n",
    "\n",
    "model = Model(n_users, n_items, n_factors=8)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # prints the parameters who's changes will be recorded\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "\n",
    "# enable GPU if you have a GPU\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# MSE loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# ADAM optimizier\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train data\n",
    "train_set = MovieDataset(ratings_df)\n",
    "train_loader = DataLoader(train_set, 128, shuffle=True)\n",
    "\n",
    "for it in range(num_epochs):\n",
    "    losses = []\n",
    "    for x, y in train_loader:\n",
    "        if cuda:\n",
    "            x, y = x.cuda(), y.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        loss = loss_fn(outputs.squeeze(), y.type(torch.float32))\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"iter #{}\".format(it), \"Loss:\", sum(losses) / len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_factors.weight tensor([[ 1.4406,  1.5085,  1.3647,  ...,  1.3247,  1.1655,  1.8264],\n",
      "        [ 0.9310,  1.1197, -0.0258,  ...,  1.1496,  1.8208,  1.7693],\n",
      "        [ 1.1364,  1.7194,  0.7117,  ..., -2.6151,  1.3925, -0.2541],\n",
      "        ...,\n",
      "        [ 0.3648,  0.5481,  1.3855,  ..., -0.6589,  0.6806,  1.1321],\n",
      "        [ 0.5329,  1.3394,  1.0249,  ...,  0.6849,  1.2605,  1.1007],\n",
      "        [ 1.8096,  1.5428,  1.3131,  ...,  0.4340,  0.8188,  0.4304]])\n",
      "movie_factors.weight tensor([[0.5541, 0.4625, 0.3197,  ..., 0.6063, 0.3152, 0.3232],\n",
      "        [0.3978, 0.4120, 0.4124,  ..., 0.6047, 0.3821, 0.6489],\n",
      "        [0.6360, 0.3837, 0.3564,  ..., 0.3592, 0.7338, 0.6272],\n",
      "        ...,\n",
      "        [0.4305, 0.4024, 0.4302,  ..., 0.4395, 0.4208, 0.3973],\n",
      "        [0.4053, 0.4127, 0.4042,  ..., 0.4353, 0.4248, 0.3825],\n",
      "        [0.4924, 0.5075, 0.4913,  ..., 0.4938, 0.5000, 0.3327]])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster #0\n",
      "\t Go Fish (1994)\n",
      "\t Chungking Express (Chung Hing sam lam) (1994)\n",
      "\t On Her Majesty's Secret Service (1969)\n",
      "\t Three Colors: Blue (Trois couleurs: Bleu) (1993)\n",
      "\t Bubba Ho-tep (2002)\n",
      "\t Substitute, The (1996)\n",
      "\t Insider, The (1999)\n",
      "\t Money Train (1995)\n",
      "\t Emma (1996)\n",
      "\t Death and the Maiden (1994)\n",
      "Cluster #1\n",
      "\t Poetic Justice (1993)\n",
      "\t Lost Weekend, The (1945)\n",
      "\t Don Juan DeMarco (1995)\n",
      "\t Being Human (1993)\n",
      "\t For Whom the Bell Tolls (1943)\n",
      "\t Bread and Chocolate (Pane e cioccolata) (1973)\n",
      "\t Kissed (1996)\n",
      "\t Philadelphia (1993)\n",
      "\t Inspector General, The (1949)\n",
      "\t Jefferson in Paris (1995)\n",
      "Cluster #2\n",
      "\t Philadelphia Story, The (1940)\n",
      "\t Afterglow (1997)\n",
      "\t Unforgiven (1992)\n",
      "\t Candleshoe (1977)\n",
      "\t Empire (2002)\n",
      "\t Blue in the Face (1995)\n",
      "\t Penny Serenade (1941)\n",
      "\t Untouchables, The (1987)\n",
      "\t Battle Royale (Batoru rowaiaru) (2000)\n",
      "\t Sunset Blvd. (a.k.a. Sunset Boulevard) (1950)\n",
      "Cluster #3\n",
      "\t Secret of Roan Inish, The (1994)\n",
      "\t Dangerous Minds (1995)\n",
      "\t Run Silent Run Deep (1958)\n",
      "\t National Lampoon's Senior Trip (1995)\n",
      "\t Puppet Masters, The (1994)\n",
      "\t New Guy, The (2002)\n",
      "\t Foxfire (1996)\n",
      "\t Love and Death (1975)\n",
      "\t Poseidon Adventure, The (1972)\n",
      "\t Naked Gun: From the Files of Police Squad!, The (1988)\n",
      "Cluster #4\n",
      "\t Perfect World, A (1993)\n",
      "\t How to Make an American Quilt (1995)\n",
      "\t Robin Hood: Men in Tights (1993)\n",
      "\t Color of Night (1994)\n",
      "\t Mixed Nuts (1994)\n",
      "\t Some Kind of Wonderful (1987)\n",
      "\t Spy Who Loved Me, The (1977)\n",
      "\t Road Trip (2000)\n",
      "\t Down to Earth (2001)\n",
      "\t Idiots, The (Idioterne) (1998)\n",
      "Cluster #5\n",
      "\t Making Mr. Right (1987)\n",
      "\t Ref, The (1994)\n",
      "\t Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
      "\t Living in Oblivion (1995)\n",
      "\t Safe (1995)\n",
      "\t Speechless (1994)\n",
      "\t Ran (1985)\n",
      "\t American Werewolf in London, An (1981)\n",
      "\t Immortal Beloved (1994)\n",
      "\t Jackie Brown (1997)\n",
      "Cluster #6\n",
      "\t Miracle on 34th Street (1994)\n",
      "\t Singin' in the Rain (1952)\n",
      "\t Some Like It Hot (1959)\n",
      "\t Piano, The (1993)\n",
      "\t Orlando (1992)\n",
      "\t What's Eating Gilbert Grape (1993)\n",
      "\t Swimming with Sharks (1995)\n",
      "\t Vanya on 42nd Street (1994)\n",
      "\t Night of the Living Dead (1968)\n",
      "\t Queen Margot (Reine Margot, La) (1994)\n",
      "Cluster #7\n",
      "\t Super Mario Bros. (1993)\n",
      "\t Unlawful Entry (1992)\n",
      "\t Friday the 13th Part V: A New Beginning (1985)\n",
      "\t NeverEnding Story III, The (1994)\n",
      "\t Crow: Salvation, The (2000)\n",
      "\t Everybody's Famous! (Iedereen beroemd!) (2000)\n",
      "\t Godzilla (1998)\n",
      "\t To Gillian on Her 37th Birthday (1996)\n",
      "\t Undercover Blues (1993)\n",
      "\t Extreme Ops (2002)\n",
      "Cluster #8\n",
      "\t Sixteen Candles (1984)\n",
      "\t Journey of Natty Gann, The (1985)\n",
      "\t My Dog Skip (1999)\n",
      "\t Out Cold (2001)\n",
      "\t Funny Face (1957)\n",
      "\t Real Genius (1985)\n",
      "\t Babe, The (1992)\n",
      "\t Summer School (1987)\n",
      "\t Wonderland (2003)\n",
      "\t Gross Anatomy (a.k.a. A Cut Above) (1989)\n",
      "Cluster #9\n",
      "\t Just Cause (1995)\n",
      "\t Hate (Haine, La) (1995)\n",
      "\t Restoration (1995)\n",
      "\t Gaslight (1944)\n",
      "\t Jennifer 8 (1992)\n",
      "\t Say Anything... (1989)\n",
      "\t Enemy of the State (1998)\n",
      "\t Space Cowboys (2000)\n",
      "\t Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "\t Glory (1989)\n"
     ]
    }
   ],
   "source": [
    "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
    "trained_movie_embeddings = model.movie_factors.weight.data.cpu().numpy()\n",
    "from sklearn.cluster import KMeans\n",
    "# Fit the clusters based on the movie weights\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(trained_movie_embeddings)\n",
    "for cluster in range(10):\n",
    "  print(\"Cluster #{}\".format(cluster))\n",
    "  movs = []\n",
    "  for movidx in np.where(kmeans.labels_ == cluster)[0]:\n",
    "    movid = train_set.lookup[movidx]\n",
    "    # print(ratings_df.loc[ratings_df['movieId']==movid].count())\n",
    "    rat_count = ratings_df.loc[ratings_df['movieId']==movid].count()[\"userId\"]\n",
    "    movs.append((movie_names[movid], rat_count))\n",
    "  for mov in sorted(movs, key=lambda tup: tup[1], reverse=True)[:10]:\n",
    "    print(\"\\t\", mov[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
